{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAAvDTEb3cBh",
    "outputId": "04885bf6-b991-443f-ff9b-6019c29d1f24",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 13:50:44.151915: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-03 13:50:44.151948: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/renan/√Årea de Trabalho/research-project/py37/lib/python3.7/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import timeit\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import itertools\n",
    "import datetime \n",
    "import os\n",
    "import pickle\n",
    "from operator import itemgetter\n",
    "from scipy.signal import cheby2, resample, sosfilt\n",
    "from scipy import signal\n",
    "from scipy.io import loadmat, savemat\n",
    "from tensorly import tensor as tensor_tly\n",
    "from tensorly import norm, dot\n",
    "from tensorly.decomposition import tucker\n",
    "from tensorly.tucker_tensor import tucker_to_tensoravailable_databases\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.regularizers import l2, l1_l2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ecg_plot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Database(object):\n",
    "    def __init__(self, path, leads):\n",
    "        self.path = path\n",
    "        self.leads = leads\n",
    "        self.freq = 0\n",
    "        self.headers_path = []\n",
    "        self.recordings_path = []\n",
    "\n",
    "\n",
    "    def extract_from_drive(self):\n",
    "        if not os.path.exists(self.path):\n",
    "            with tarfile.open(self.path) as zip_file:\n",
    "                zip_file.extractall()\n",
    "    \n",
    "\n",
    "    def load_hea_file(self, i):\n",
    "        with open(self.headers_path[i], 'r') as f:\n",
    "            hea_file = f.read()\n",
    "        return hea_file\n",
    "\n",
    "\n",
    "    def get_frequency(self):\n",
    "\n",
    "\n",
    "\n",
    "    def load_paths(self):\n",
    "        for f in sorted(os.listdir(self.path)):\n",
    "            root, extension = os.path.splitext(f)\n",
    "            if not root.startswith('.') and extension=='.hea':\n",
    "                header_db_file = os.path.join(self.path, root + '.hea')\n",
    "                recording_db_file = os.path.join(self.path, root + '.mat')\n",
    "                if os.path.isfile(header_db_file) and os.path.isfile(recording_db_file):\n",
    "                    self.headers_path.append(header_db_file)\n",
    "                    self.recordings_path.append(recording_db_file)\n",
    "        print(f'Found {len(self.headers_path)} recordings in {self.path}.')\n",
    "    \n",
    "\n",
    "class Diagnostic(object):\n",
    "    diagnostics = []\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def append_diagnostic(cls, diagnostic):\n",
    "        cls.diagnostics.append(diagnostic)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def get_diagnostics(cls):\n",
    "        return cls.diagnostics\n",
    "\n",
    "\n",
    "    def __init__(self, diag_name, abbrev, code):\n",
    "        self.name = diag_name\n",
    "        self.abbrev = abbrev\n",
    "        self.code = code\n",
    "    \n",
    "    \n",
    "\n",
    "class DiagnosticDatabase(object):\n",
    "    databases = []\n",
    "\n",
    "    @classmethod\n",
    "    def append_database(cls, diag_db):\n",
    "        cls.databases.append(diag_db)     \n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def get_df_recordings(cls):    \n",
    "        total_recs = []\n",
    "        for diag_db in cls.databases:\n",
    "                recs = [rec.__dict__ for rec in diag_db.recordings_diag]\n",
    "                recs = [dict(rec, db=diag_db.db.path.split('/')[2], diagnostic=diag_db.diagnostic.abbrev) for rec in recs]\n",
    "                total_recs.append(recs)\n",
    "        total_recs = [rec for db in total_recs for rec in db]\n",
    "        dataframe = pd.DataFrame(total_recs)\n",
    "        return dataframe\n",
    "\n",
    "\n",
    "    def __init__(self, diag_origin, db_origin):\n",
    "        self.diagnostic = diag_origin\n",
    "        self.db = db_origin\n",
    "        self.headers_diag_path = []\n",
    "        self.recordings_diag = []\n",
    "\n",
    "\n",
    "    def get_labels(self, header):\n",
    "        labels = list()\n",
    "        for l in header.split('\\n'):\n",
    "            if l.startswith('#Dx'):\n",
    "                try:\n",
    "                    entries = l.split(': ')[1].split(',')\n",
    "                    for entry in entries:\n",
    "                        labels.append(entry.strip())\n",
    "                except:\n",
    "                    pass\n",
    "        return labels\n",
    "\n",
    "\n",
    "    def get_leads(self,header):\n",
    "        leads = list()\n",
    "        for i, l in enumerate(header.split('\\n')):\n",
    "            entries = l.split(' ')\n",
    "            if i==0:\n",
    "                num_leads = int(entries[1])\n",
    "            elif i<=num_leads:\n",
    "                leads.append(entries[-1])\n",
    "            else:\n",
    "                break\n",
    "        return tuple(leads)\n",
    "\n",
    "\n",
    "    def choose_leads(self, recording, header, leads):\n",
    "        num_leads = len(leads)\n",
    "        num_samples = np.shape(recording)[1]\n",
    "        chosen_recording = np.zeros((num_leads, num_samples), recording.dtype)\n",
    "        available_leads = self.get_leads(header)\n",
    "        for i, lead in enumerate(leads):\n",
    "            if lead in available_leads:\n",
    "                j = available_leads.index(lead)\n",
    "                chosen_recording[i, :] = recording[j, :]\n",
    "        return chosen_recording\n",
    "\n",
    "\n",
    "    def plot_ecg(self, index):\n",
    "        ecg_plot.plot(self.rsp_cut_recordings_diag[index]/1000, sample_rate=self.db.freq/2, title='')\n",
    "        ecg_plot.show()\n",
    "\n",
    "\n",
    "class Record():\n",
    "    def __init__(self, filename, inf, sup, data):\n",
    "        self.filename = filename\n",
    "        self.inf = inf\n",
    "        self.sup = sup\n",
    "        self.data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QzVMxEyADvRx",
    "outputId": "34f8d11e-f247-4cba-e941-dd192d6d900d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WFDB_Ga', 'WFDB_Ningbo', 'WFDB_CPSC2018_2', 'WFDB_CPSC2018', 'WFDB_PTBXL', 'WFDB_ChapmanShaoxing']\n",
      "Found 10344 recordings in ../databases/WFDB_Ga.\n",
      "Frequency: 500.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_124795/3737403913.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Recordings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiag_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiag_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_hea_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiag_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdiag_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagnostic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_124795/177620161.py\u001b[0m in \u001b[0;36mload_hea_file\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_hea_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mhea_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhea_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DiagnosticDatabase.databases = []\n",
    "Diagnostic.diagnostics = []\n",
    "\n",
    "af_diag = Diagnostic('Atrial Fibrilation', 'AF', '164889003')\n",
    "sr_diag = Diagnostic('Sinus Rhythm', 'SR', '426783006')\n",
    "Diagnostic.append_diagnostic(af_diag)\n",
    "Diagnostic.append_diagnostic(sr_diag)\n",
    "\n",
    "path_folder = '../databases/'\n",
    "available_databases = os.listdir(path_folder)\n",
    "available_databases.remove('.gitkeep')\n",
    "available_databases.remove('WFDB_PTB')\n",
    "available_databases.remove('WFDB_StPetersburg')\n",
    "print(available_databases)\n",
    "\n",
    "available_databases = list(map(lambda db: path_folder + db, available_databases))\n",
    "leads = ('I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6')\n",
    "\n",
    "# Databases\n",
    "for path in available_databases:\n",
    "    folder = re.search('databases/(.*)', path).group(1)\n",
    "    db = Database(path, leads)\n",
    "    db.load_paths()\n",
    "    db.get_frequency()\n",
    "        \n",
    "    # Diagnostics\n",
    "    for diag in Diagnostic.get_diagnostics():\n",
    "        diag_db = DiagnosticDatabase(diag, db)\n",
    "        \n",
    "        # Recordings\n",
    "        for i, header_path in enumerate(diag_db.db.headers_path):\n",
    "            header = diag_db.db.load_hea_file(i)\n",
    "            labels = diag_db.get_labels(header)\n",
    "            if diag_db.diagnostic.code in labels:\n",
    "                # Get record\n",
    "                rec_file = header_path.replace('hea', 'mat')\n",
    "                recording = loadmat(rec_file)['val']\n",
    "                recording = diag_db.choose_leads(recording, header, diag_db.db.leads)\n",
    "                recording = np.array(recording, dtype=np.float64)\n",
    "                \n",
    "                # Filtering\n",
    "                sos = signal.cheby2(12, 20, [0.35, 70], \n",
    "                                    'bandpass', \n",
    "                                    fs= diag_db.db.freq, \n",
    "                                    output='sos')\n",
    "                for lead in range(0, recording.shape[0]):\n",
    "                        to_filt = np.array(recording[lead, :], dtype=np.float64)\n",
    "                        filtered = signal.sosfilt(sos, to_filt)\n",
    "                        recording[lead, :] = np.array(filtered, dtype=np.float64)\n",
    "\n",
    "                # Resample record to 250Hz\n",
    "                new_freq = 250\n",
    "                time_rec = len(recording[0])/diag_db.db.freq\n",
    "                n_samples = int(time_rec*new_freq)\n",
    "                recording = resample(recording, n_samples, axis=1)\n",
    "\n",
    "                # Cut record in 250 samples\n",
    "                interval = 250\n",
    "                size_rec = recording.shape[1]\n",
    "                samples_rec = math.floor(size_rec/interval)\n",
    "                for i in range(0, samples_rec):\n",
    "                    inf = i*interval\n",
    "                    sup = ((i+1)*interval)\n",
    "                    recording_interval = np.array(list(map(lambda lead: lead[inf:sup], recording)))\n",
    "                    rec_filename = rec_file.split('/')[-1]\n",
    "                    record = Record(rec_filename, \n",
    "                                    inf, \n",
    "                                    sup, \n",
    "                                    recording_interval)\n",
    "                    diag_db.recordings_diag.append(record)\n",
    "                diag_db.headers_diag_path.append(header_path)\n",
    "        print(f' - Found {len(diag_db.headers_diag_path)} recordings for {diag_db.diagnostic.abbrev}.')\n",
    "        print(f' - Unattached {len(diag_db.recordings_diag)} intervals.')\n",
    "        DiagnosticDatabase.append_database(diag_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL: 343213 rows\n",
      "AF: 54191 rows\n",
      "SR: 289022 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "diagnostic  db                  \n",
       "AF          WFDB_CPSC2018            18306\n",
       "            WFDB_CPSC2018_2           2325\n",
       "            WFDB_ChapmanShaoxing     12730\n",
       "            WFDB_Ga                   5690\n",
       "            WFDB_PTBXL               15140\n",
       "SR          WFDB_CPSC2018            14116\n",
       "            WFDB_CPSC2018_2             61\n",
       "            WFDB_ChapmanShaoxing     13500\n",
       "            WFDB_Ga                  17435\n",
       "            WFDB_Ningbo              62990\n",
       "            WFDB_PTBXL              180920\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = DiagnosticDatabase.get_df_recordings()\n",
    "print(f\"TOTAL: {df.shape[0]} rows\")\n",
    "df = df.sample(frac=1, ignore_index=True, random_state=32)\n",
    "print(f\"AF: {str(df[df.diagnostic == 'AF'].shape[0])} rows\")\n",
    "print(f\"SR: {str(df[df.diagnostic == 'SR'].shape[0])} rows\")\n",
    "df.groupby(['diagnostic', 'db']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>inf</th>\n",
       "      <th>sup</th>\n",
       "      <th>data</th>\n",
       "      <th>db</th>\n",
       "      <th>diagnostic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q3000.mat</td>\n",
       "      <td>1000</td>\n",
       "      <td>1250</td>\n",
       "      <td>[[9.760849635141424, 12.143493689638246, 14.75...</td>\n",
       "      <td>WFDB_CPSC2018_2</td>\n",
       "      <td>AF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JS38372.mat</td>\n",
       "      <td>1750</td>\n",
       "      <td>2000</td>\n",
       "      <td>[[-8.622005688806848, -12.716669437907457, -7....</td>\n",
       "      <td>WFDB_Ningbo</td>\n",
       "      <td>SR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HR20937.mat</td>\n",
       "      <td>1000</td>\n",
       "      <td>1250</td>\n",
       "      <td>[[11.846834097678906, -5.77366283997905, -33.6...</td>\n",
       "      <td>WFDB_PTBXL</td>\n",
       "      <td>SR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HR00086.mat</td>\n",
       "      <td>1750</td>\n",
       "      <td>2000</td>\n",
       "      <td>[[12.649530402341707, 15.416020621653105, 17.1...</td>\n",
       "      <td>WFDB_PTBXL</td>\n",
       "      <td>SR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JS09370.mat</td>\n",
       "      <td>2000</td>\n",
       "      <td>2250</td>\n",
       "      <td>[[-6.000065873350305, -3.6633855946070715, -18...</td>\n",
       "      <td>WFDB_ChapmanShaoxing</td>\n",
       "      <td>AF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      filename   inf   sup                                               data  \\\n",
       "0    Q3000.mat  1000  1250  [[9.760849635141424, 12.143493689638246, 14.75...   \n",
       "1  JS38372.mat  1750  2000  [[-8.622005688806848, -12.716669437907457, -7....   \n",
       "2  HR20937.mat  1000  1250  [[11.846834097678906, -5.77366283997905, -33.6...   \n",
       "3  HR00086.mat  1750  2000  [[12.649530402341707, 15.416020621653105, 17.1...   \n",
       "4  JS09370.mat  2000  2250  [[-6.000065873350305, -3.6633855946070715, -18...   \n",
       "\n",
       "                     db diagnostic  \n",
       "0       WFDB_CPSC2018_2         AF  \n",
       "1           WFDB_Ningbo         SR  \n",
       "2            WFDB_PTBXL         SR  \n",
       "3            WFDB_PTBXL         SR  \n",
       "4  WFDB_ChapmanShaoxing         AF  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide dataframe in pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "div = int(len(df)/13)\n",
    "\n",
    "for i in range(0, 13):\n",
    "    start = div * i \n",
    "    final = div * (i + 1)\n",
    "    df.iloc[start:final].to_pickle(f'../workdata/mu/part_{i}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart kernel (Flush RAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os._exit(00)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Experiments.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
