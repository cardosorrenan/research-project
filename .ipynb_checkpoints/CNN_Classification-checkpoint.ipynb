{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8381ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "334d8c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(343213, 6)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('2022-01-31_23_27_05.csv')\n",
    "df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15ab2ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL: 343213 rows\n",
      "AF: 54191 rows\n",
      "SR: 289022 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "diagnostic  db                  \n",
       "AF          WFDB_CPSC2018            18306\n",
       "            WFDB_CPSC2018_2           2325\n",
       "            WFDB_ChapmanShaoxing     12730\n",
       "            WFDB_Ga                   5690\n",
       "            WFDB_PTBXL               15140\n",
       "SR          WFDB_CPSC2018            14116\n",
       "            WFDB_CPSC2018_2             61\n",
       "            WFDB_ChapmanShaoxing     13500\n",
       "            WFDB_Ga                  17435\n",
       "            WFDB_Ningbo              62990\n",
       "            WFDB_PTBXL              180920\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"TOTAL: {df.shape[0]} rows\")\n",
    "print(f\"AF: {str(df[df.diagnostic == 'AF'].shape[0])} rows\")\n",
    "print(f\"SR: {str(df[df.diagnostic == 'SR'].shape[0])} rows\")\n",
    "df.groupby(['diagnostic', 'db']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cd06b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = df.groupby('diagnostic')\n",
    "g = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e3c1b186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL: 108382 rows\n",
      "AF: 54191 rows\n",
      "SR: 54191 rows\n"
     ]
    }
   ],
   "source": [
    "print(f\"TOTAL: {g.shape[0]} rows\")\n",
    "print(f\"AF: {str(g[g.diagnostic == 'AF'].shape[0])} rows\")\n",
    "print(f\"SR: {str(g[g.diagnostic == 'SR'].shape[0])} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9ea94f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recordings_AF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_179295/2269763284.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecordings_AF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecordings_SR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecordings_AF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecordings_SR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'recordings_AF' is not defined"
     ]
    }
   ],
   "source": [
    "X = np.vstack((recordings_AF, recordings_SR))\n",
    "y = np.hstack((np.ones(len(recordings_AF)).T, np.zeros(len(recordings_SR)).T))\n",
    "y = keras.utils.to_categorical(y, 2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train = X_train.reshape([*X_train.shape[0:3], 1])\n",
    "X_test = X_test.reshape([*X_test.shape[0:3], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1426ce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, (4, 4), input_shape=X_train.shape[1:], padding='same', kernel_regularizer=l1_l2(0.0001, 0.0001), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (4, 4), kernel_regularizer=l2(0.0001), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((4, 4)))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Conv2D(256, (4, 4), kernel_regularizer=l2(0.0001), padding='same', activation='relu'))\n",
    "model.add(Conv2D(256, (4, 4), kernel_regularizer=l2(0.0001), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, kernel_regularizer=l2(0.0001), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "callbacks_list = [\n",
    "  keras.callbacks.ModelCheckpoint(\n",
    "      filepath='model.h5',\n",
    "      monitor='val_loss', save_best_only=True, verbose=1),\n",
    "  keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model = model.fit(X_train, y_train,\n",
    "               batch_size=124,\n",
    "               epochs=10,\n",
    "               callbacks = callbacks_list,\n",
    "               verbose=1,\n",
    "               validation_data=(X_test, y_test))\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(16,8))\n",
    "ax[0].plot(model.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(model.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(model.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(model.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('/content/gdrive/MyDrive/Pesquisa Mestrado/Parte II (Novembro-)/Databases/Models/Model1.h5')\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
    "print (\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
